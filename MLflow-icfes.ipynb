{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b168fd51-91b4-420a-9c22-14d5419fe934",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importe MLflow, ketas y tensorflow\n",
    "import mlflow \n",
    "import mlflow.keras\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tk\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42186ec5-2359-4e32-a47b-91f27c82ca2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--momentum', '-m'], dest='momentum', nargs=None, const=None, default=0.85, type=<class 'float'>, choices=None, required=False, help=None, metavar=None)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usaremos argparse para pasarle argumentos a las funciones de entrenamiento\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Entrenamiento de una red feed-forward para el problema de clasificación con datos icfes en TensorFlow/Keras')\n",
    "parser.add_argument('--batch_size', '-b', type=int, default=32)\n",
    "parser.add_argument('--epochs', '-e', type=int, default=5)\n",
    "parser.add_argument('--learning_rate', '-l', type=float, default=0.05)\n",
    "parser.add_argument('--num_hidden_units', '-n', type=int, default=8)\n",
    "parser.add_argument('--num_hidden_layers', '-N', type=int, default=1)\n",
    "parser.add_argument('--dropout', '-d', type=float, default=0.25)\n",
    "parser.add_argument('--momentum', '-m', type=float, default=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd447bfc-c3db-45b2-9f90-e80a4d262431",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7060bbb5-e040-446c-8477-6189ce5d81a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_optimizer():\n",
    "    \"\"\"\n",
    "    :return: Keras optimizer\n",
    "    \"\"\"\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=args.learning_rate,momentum=args.momentum, nesterov=True)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c85c7f17-0c97-4eae-80f4-cb06b8ea90f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtenemos el dataset icfes\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Cargar archivo CSV\n",
    "data_spark = spark.read.option(\"header\",\"true\").csv(\"/FileStore/tables/Icfes_limpio3-2.csv\")\n",
    "data = data_spark.toPandas()\n",
    "# Codificación de variables categóricas\n",
    "data_encoded = pd.get_dummies(data, columns=['cole_depto_ubicacion', 'cole_area_ubicacion','cole_naturaleza','cole_bilingue','fami_estratovivienda','presento_todas_las_areas'])\n",
    "# Separar variables independientes (X) y dependientes (y)\n",
    "X = data_encoded.drop('NIVEL',axis=1)\n",
    "Y = data_encoded['NIVEL']\n",
    "# Escalado de variables numéricas\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividir en conjunto de entrenamiento y prueba\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbb242b2-f9f1-4c44-9c98-8a615e688736",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Esta función define una corrida del modelo, con entrenamiento y \n",
    "# registro en MLflow\n",
    "def run_mlflow(run_name=\"MLflow icfes\"):\n",
    "    # Iniciamos una corrida de MLflow\n",
    "    mlflow.start_run(run_name=run_name)\n",
    "    run = mlflow.active_run()\n",
    "    # MLflow asigna un ID al experimento y a la corrida\n",
    "    experimentID = run.info.experiment_id\n",
    "    runID = run.info.run_uuid\n",
    "    # reistro automáticos de las métricas de keras\n",
    "    mlflow.keras.autolog()\n",
    "    model = models.Sequential()  \n",
    "    # La primera capa de la red\n",
    "    #model.add(Dense(53, input_dim=X.shape[1], activation='relu'))   \n",
    "    model.add(layers.Flatten(input_shape=x_train[0].shape))\n",
    "    # Agregamos capas ocultas a la red\n",
    "    # en los argumentos: --num_hidden_layers o -N \n",
    "    for n in range(0, args.num_hidden_layers):\n",
    "        # agregamos una capa densa (completamente conectada) con función de activación relu\n",
    "        model.add(layers.Dense(args.num_hidden_units, activation=tf.nn.relu))\n",
    "        # agregamos dropout como método de regularización para aleatoriamente descartar una capa\n",
    "        # si los gradientes son muy pequeños\n",
    "        model.add(layers.Dropout(args.dropout))\n",
    "        # capa final con 1 nodos de salida y activación softmax \n",
    "        model.add(layers.Dense(1, activation=tf.nn.softmax))\n",
    "        # Use \n",
    "        # https://keras.io/optimizers/\n",
    "        optimizer = get_optimizer()\n",
    "\n",
    "    # compilamos el modelo y definimos la función de pérdida  \n",
    "    # otras funciones de pérdida comunes para problemas de clasificación\n",
    "    # 1. sparse_categorical_crossentropy\n",
    "    # 2. binary_crossentropy\n",
    "    model.compile(optimizer=optimizer,\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    # entrenamos el modelo\n",
    "    print(\"-\" * 100)\n",
    "    model.fit(x_train, y_train, epochs=args.epochs, batch_size=args.batch_size)\n",
    "    # evaluamos el modelo\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "    mlflow.end_run(status='FINISHED')\n",
    "    return (experimentID, runID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f64a856-253d-4d60-a7e5-5dd441c91f89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(53,)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74154f47-5a1c-4b57-9f63-4d8d9be7087e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/25 20:09:25 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'Series' object has no attribute 'flatten'\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mUnimplementedError\u001B[0m                        Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2246144502270323>, line 3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# corrida con parámetros diferentes a los por defecto\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m args \u001B[38;5;241m=\u001B[39m parser\u001B[38;5;241m.\u001B[39mparse_args([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--batch_size\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m256\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m--epochs\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m8\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
       "\u001B[0;32m----> 3\u001B[0m (experimentID, runID) \u001B[38;5;241m=\u001B[39m run_mlflow()\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMLflow Run completed with run_id \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m and experiment_id \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(runID, experimentID))\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(tf\u001B[38;5;241m.\u001B[39m__version__)\n",
       "\n",
       "File \u001B[0;32m<command-2246144502270322>, line 40\u001B[0m, in \u001B[0;36mrun_mlflow\u001B[0;34m(run_name)\u001B[0m\n",
       "\u001B[1;32m     38\u001B[0m \u001B[38;5;66;03m# entrenamos el modelo\u001B[39;00m\n",
       "\u001B[1;32m     39\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m100\u001B[39m)\n",
       "\u001B[0;32m---> 40\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# evaluamos el modelo\u001B[39;00m\n",
       "\u001B[1;32m     42\u001B[0m test_loss, test_acc \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mevaluate(x_test, y_test, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:550\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    540\u001B[0m try_log_autologging_event(\n",
       "\u001B[1;32m    541\u001B[0m     AutologgingEventLogger\u001B[38;5;241m.\u001B[39mget_logger()\u001B[38;5;241m.\u001B[39mlog_patch_function_start,\n",
       "\u001B[1;32m    542\u001B[0m     session,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    546\u001B[0m     kwargs,\n",
       "\u001B[1;32m    547\u001B[0m )\n",
       "\u001B[1;32m    549\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m patch_is_class:\n",
       "\u001B[0;32m--> 550\u001B[0m     \u001B[43mpatch_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcall_original\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    551\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    552\u001B[0m     patch_function(call_original, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:168\u001B[0m, in \u001B[0;36mPatchFunction.call\u001B[0;34m(cls, original, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    166\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n",
       "\u001B[1;32m    167\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall\u001B[39m(\u001B[38;5;28mcls\u001B[39m, original, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n",
       "\u001B[0;32m--> 168\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moriginal\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:179\u001B[0m, in \u001B[0;36mPatchFunction.__call__\u001B[0;34m(self, original, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    175\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_on_exception(e)\n",
       "\u001B[1;32m    176\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m    177\u001B[0m     \u001B[38;5;66;03m# Regardless of what happens during the `_on_exception` callback, reraise\u001B[39;00m\n",
       "\u001B[1;32m    178\u001B[0m     \u001B[38;5;66;03m# the original implementation exception once the callback completes\u001B[39;00m\n",
       "\u001B[0;32m--> 179\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:172\u001B[0m, in \u001B[0;36mPatchFunction.__call__\u001B[0;34m(self, original, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    170\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, original, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n",
       "\u001B[1;32m    171\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 172\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_patch_implementation\u001B[49m\u001B[43m(\u001B[49m\u001B[43moriginal\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    173\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mException\u001B[39;00m, \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    174\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:230\u001B[0m, in \u001B[0;36mwith_managed_run.<locals>.PatchWithManagedRun._patch_implementation\u001B[0;34m(self, original, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    227\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m mlflow\u001B[38;5;241m.\u001B[39mactive_run():\n",
       "\u001B[1;32m    228\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmanaged_run \u001B[38;5;241m=\u001B[39m create_managed_run()\n",
       "\u001B[0;32m--> 230\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_patch_implementation\u001B[49m\u001B[43m(\u001B[49m\u001B[43moriginal\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmanaged_run:\n",
       "\u001B[1;32m    233\u001B[0m     mlflow\u001B[38;5;241m.\u001B[39mend_run(RunStatus\u001B[38;5;241m.\u001B[39mto_string(RunStatus\u001B[38;5;241m.\u001B[39mFINISHED))\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/mlflow/tensorflow/__init__.py:1268\u001B[0m, in \u001B[0;36mautolog.<locals>.FitPatch._patch_implementation\u001B[0;34m(self, original, inst, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1261\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m   1262\u001B[0m         _logger\u001B[38;5;241m.\u001B[39mwarning(\n",
       "\u001B[1;32m   1263\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to log training dataset information to \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   1264\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMLflow Tracking. Reason: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   1265\u001B[0m             e,\n",
       "\u001B[1;32m   1266\u001B[0m         )\n",
       "\u001B[0;32m-> 1268\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43moriginal\u001B[49m\u001B[43m(\u001B[49m\u001B[43minst\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1270\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m log_models:\n",
       "\u001B[1;32m   1271\u001B[0m     _log_keras_model(history, args)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:533\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001B[0;34m(*og_args, **og_kwargs)\u001B[0m\n",
       "\u001B[1;32m    530\u001B[0m         original_result \u001B[38;5;241m=\u001B[39m original(\u001B[38;5;241m*\u001B[39m_og_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_og_kwargs)\n",
       "\u001B[1;32m    531\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m original_result\n",
       "\u001B[0;32m--> 533\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcall_original_fn_with_event_logging\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_original_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mog_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mog_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:468\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001B[0;34m(original_fn, og_args, og_kwargs)\u001B[0m\n",
       "\u001B[1;32m    459\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m    460\u001B[0m     try_log_autologging_event(\n",
       "\u001B[1;32m    461\u001B[0m         AutologgingEventLogger\u001B[38;5;241m.\u001B[39mget_logger()\u001B[38;5;241m.\u001B[39mlog_original_function_start,\n",
       "\u001B[1;32m    462\u001B[0m         session,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    466\u001B[0m         og_kwargs,\n",
       "\u001B[1;32m    467\u001B[0m     )\n",
       "\u001B[0;32m--> 468\u001B[0m     original_fn_result \u001B[38;5;241m=\u001B[39m \u001B[43moriginal_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mog_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mog_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    470\u001B[0m     try_log_autologging_event(\n",
       "\u001B[1;32m    471\u001B[0m         AutologgingEventLogger\u001B[38;5;241m.\u001B[39mget_logger()\u001B[38;5;241m.\u001B[39mlog_original_function_success,\n",
       "\u001B[1;32m    472\u001B[0m         session,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    476\u001B[0m         og_kwargs,\n",
       "\u001B[1;32m    477\u001B[0m     )\n",
       "\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m original_fn_result\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:530\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001B[0;34m(*_og_args, **_og_kwargs)\u001B[0m\n",
       "\u001B[1;32m    522\u001B[0m \u001B[38;5;66;03m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001B[39;00m\n",
       "\u001B[1;32m    523\u001B[0m \u001B[38;5;66;03m# during original function execution, even if silent mode is enabled\u001B[39;00m\n",
       "\u001B[1;32m    524\u001B[0m \u001B[38;5;66;03m# (`silent=True`), since these warnings originate from the ML framework\u001B[39;00m\n",
       "\u001B[1;32m    525\u001B[0m \u001B[38;5;66;03m# or one of its dependencies and are likely relevant to the caller\u001B[39;00m\n",
       "\u001B[1;32m    526\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m set_non_mlflow_warnings_behavior_for_current_thread(\n",
       "\u001B[1;32m    527\u001B[0m     disable_warnings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n",
       "\u001B[1;32m    528\u001B[0m     reroute_warnings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n",
       "\u001B[1;32m    529\u001B[0m ):\n",
       "\u001B[0;32m--> 530\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m \u001B[43moriginal\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m_og_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m_og_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    531\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m original_result\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
       "\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n",
       "\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n",
       "\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n",
       "\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m     51\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n",
       "\u001B[0;32m---> 52\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m pywrap_tfe\u001B[38;5;241m.\u001B[39mTFE_Py_Execute(ctx\u001B[38;5;241m.\u001B[39m_handle, device_name, op_name,\n",
       "\u001B[1;32m     53\u001B[0m                                       inputs, attrs, num_outputs)\n",
       "\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m     55\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mUnimplementedError\u001B[0m: Graph execution error:\n",
       "\n",
       "Detected at node 'Cast_1' defined at (most recent call last):\n",
       "    File \"/databricks/python_shell/scripts/db_ipykernel_launcher.py\", line 145, in <module>\n",
       "      app.start()\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 712, in start\n",
       "      self.io_loop.start()\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
       "      self.asyncio_loop.run_forever()\n",
       "    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
       "      self._run_once()\n",
       "    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
       "      handle._run()\n",
       "    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
       "      self._context.run(self._callback, *self._args)\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
       "      await self.process_one()\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
       "      await dispatch(*args)\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
       "      await result\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
       "      reply_content = await reply_content\n",
       "    File \"/databricks/python_shell/dbruntime/DatabricksShell.py\", line 98, in do_execute\n",
       "      reply_content = await super().do_execute(*args, **kwargs)\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
       "      res = shell.run_cell(\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
       "      return super().run_cell(*args, **kwargs)\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n",
       "      result = self._run_cell(\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n",
       "      result = runner(coro)\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
       "      coro.send(None)\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n",
       "      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n",
       "      if await self.run_code(code, result, async_=asy):\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n",
       "      exec(code_obj, self.user_global_ns, self.user_ns)\n",
       "    File \"/root/.ipykernel/982/command-2246144502270323-1672747808\", line 3, in <module>\n",
       "      (experimentID, runID) = run_mlflow()\n",
       "    File \"/root/.ipykernel/982/command-2246144502270322-1499505420\", line 40, in run_mlflow\n",
       "      model.fit(x_train, y_train, epochs=args.epochs, batch_size=args.batch_size)\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py\", line 550, in safe_patch_function\n",
       "      patch_function.call(call_original, *args, **kwargs)\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py\", line 168, in call\n",
       "      return cls().__call__(original, *args, **kwargs)\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py\", line 172, in __call__\n",
       "      return self._patch_implementation(original, *args, **kwargs)\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py\", line 230, in _patch_implementation\n",
       "      result = super()._patch_implementation(original, *args, **kwargs)\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/mlflow/tensorflow/__init__.py\", line 1268, in _patch_implementation\n",
       "      history = original(inst, *args, **kwargs)\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py\", line 533, in call_original\n",
       "      return call_original_fn_with_event_logging(_original_fn, og_args, og_kwargs)\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py\", line 468, in call_original_fn_with_event_logging\n",
       "      original_fn_result = original_fn(*og_args, **og_kwargs)\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py\", line 530, in _original_fn\n",
       "      original_result = original(*_og_args, **_og_kwargs)\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
       "      return fn(*args, **kwargs)\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/keras/engine/training.py\", line 1650, in fit\n",
       "      tmp_logs = self.train_function(iterator)\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function\n",
       "      return step_function(self, iterator)\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function\n",
       "      outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step\n",
       "      outputs = model.train_step(data)\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/keras/engine/training.py\", line 1028, in train_step\n",
       "      return self.compute_metrics(x, y, y_pred, sample_weight)\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/keras/engine/training.py\", line 1122, in compute_metrics\n",
       "      self.compiled_metrics.update_state(y, y_pred, sample_weight)\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 605, in update_state\n",
       "      metric_obj.update_state(y_t, y_p, sample_weight=mask)\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/keras/utils/metrics_utils.py\", line 77, in decorated\n",
       "      update_op = update_state_fn(*args, **kwargs)\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/keras/metrics/base_metric.py\", line 140, in update_state_fn\n",
       "      return ag_update_state(*args, **kwargs)\n",
       "    File \"/databricks/python/lib/python3.10/site-packages/keras/metrics/base_metric.py\", line 676, in update_state\n",
       "      y_true = tf.cast(y_true, self._dtype)\n",
       "Node: 'Cast_1'\n",
       "Cast string to float is not supported\n",
       "\t [[{{node Cast_1}}]] [Op:__inference_train_function_5023]"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mUnimplementedError\u001B[0m                        Traceback (most recent call last)\nFile \u001B[0;32m<command-2246144502270323>, line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# corrida con parámetros diferentes a los por defecto\u001B[39;00m\n\u001B[1;32m      2\u001B[0m args \u001B[38;5;241m=\u001B[39m parser\u001B[38;5;241m.\u001B[39mparse_args([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--batch_size\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m256\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m--epochs\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m8\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m----> 3\u001B[0m (experimentID, runID) \u001B[38;5;241m=\u001B[39m run_mlflow()\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMLflow Run completed with run_id \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m and experiment_id \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(runID, experimentID))\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(tf\u001B[38;5;241m.\u001B[39m__version__)\n\nFile \u001B[0;32m<command-2246144502270322>, line 40\u001B[0m, in \u001B[0;36mrun_mlflow\u001B[0;34m(run_name)\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;66;03m# entrenamos el modelo\u001B[39;00m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m100\u001B[39m)\n\u001B[0;32m---> 40\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# evaluamos el modelo\u001B[39;00m\n\u001B[1;32m     42\u001B[0m test_loss, test_acc \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mevaluate(x_test, y_test, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:550\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    540\u001B[0m try_log_autologging_event(\n\u001B[1;32m    541\u001B[0m     AutologgingEventLogger\u001B[38;5;241m.\u001B[39mget_logger()\u001B[38;5;241m.\u001B[39mlog_patch_function_start,\n\u001B[1;32m    542\u001B[0m     session,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    546\u001B[0m     kwargs,\n\u001B[1;32m    547\u001B[0m )\n\u001B[1;32m    549\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m patch_is_class:\n\u001B[0;32m--> 550\u001B[0m     \u001B[43mpatch_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcall_original\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    551\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    552\u001B[0m     patch_function(call_original, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:168\u001B[0m, in \u001B[0;36mPatchFunction.call\u001B[0;34m(cls, original, *args, **kwargs)\u001B[0m\n\u001B[1;32m    166\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m    167\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall\u001B[39m(\u001B[38;5;28mcls\u001B[39m, original, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 168\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moriginal\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:179\u001B[0m, in \u001B[0;36mPatchFunction.__call__\u001B[0;34m(self, original, *args, **kwargs)\u001B[0m\n\u001B[1;32m    175\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_on_exception(e)\n\u001B[1;32m    176\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    177\u001B[0m     \u001B[38;5;66;03m# Regardless of what happens during the `_on_exception` callback, reraise\u001B[39;00m\n\u001B[1;32m    178\u001B[0m     \u001B[38;5;66;03m# the original implementation exception once the callback completes\u001B[39;00m\n\u001B[0;32m--> 179\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:172\u001B[0m, in \u001B[0;36mPatchFunction.__call__\u001B[0;34m(self, original, *args, **kwargs)\u001B[0m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, original, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    171\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 172\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_patch_implementation\u001B[49m\u001B[43m(\u001B[49m\u001B[43moriginal\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    173\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mException\u001B[39;00m, \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    174\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:230\u001B[0m, in \u001B[0;36mwith_managed_run.<locals>.PatchWithManagedRun._patch_implementation\u001B[0;34m(self, original, *args, **kwargs)\u001B[0m\n\u001B[1;32m    227\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m mlflow\u001B[38;5;241m.\u001B[39mactive_run():\n\u001B[1;32m    228\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmanaged_run \u001B[38;5;241m=\u001B[39m create_managed_run()\n\u001B[0;32m--> 230\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_patch_implementation\u001B[49m\u001B[43m(\u001B[49m\u001B[43moriginal\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmanaged_run:\n\u001B[1;32m    233\u001B[0m     mlflow\u001B[38;5;241m.\u001B[39mend_run(RunStatus\u001B[38;5;241m.\u001B[39mto_string(RunStatus\u001B[38;5;241m.\u001B[39mFINISHED))\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/mlflow/tensorflow/__init__.py:1268\u001B[0m, in \u001B[0;36mautolog.<locals>.FitPatch._patch_implementation\u001B[0;34m(self, original, inst, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1261\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1262\u001B[0m         _logger\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[1;32m   1263\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to log training dataset information to \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1264\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMLflow Tracking. Reason: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1265\u001B[0m             e,\n\u001B[1;32m   1266\u001B[0m         )\n\u001B[0;32m-> 1268\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43moriginal\u001B[49m\u001B[43m(\u001B[49m\u001B[43minst\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1270\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m log_models:\n\u001B[1;32m   1271\u001B[0m     _log_keras_model(history, args)\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:533\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001B[0;34m(*og_args, **og_kwargs)\u001B[0m\n\u001B[1;32m    530\u001B[0m         original_result \u001B[38;5;241m=\u001B[39m original(\u001B[38;5;241m*\u001B[39m_og_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_og_kwargs)\n\u001B[1;32m    531\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m original_result\n\u001B[0;32m--> 533\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcall_original_fn_with_event_logging\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_original_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mog_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mog_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:468\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001B[0;34m(original_fn, og_args, og_kwargs)\u001B[0m\n\u001B[1;32m    459\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    460\u001B[0m     try_log_autologging_event(\n\u001B[1;32m    461\u001B[0m         AutologgingEventLogger\u001B[38;5;241m.\u001B[39mget_logger()\u001B[38;5;241m.\u001B[39mlog_original_function_start,\n\u001B[1;32m    462\u001B[0m         session,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    466\u001B[0m         og_kwargs,\n\u001B[1;32m    467\u001B[0m     )\n\u001B[0;32m--> 468\u001B[0m     original_fn_result \u001B[38;5;241m=\u001B[39m \u001B[43moriginal_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mog_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mog_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    470\u001B[0m     try_log_autologging_event(\n\u001B[1;32m    471\u001B[0m         AutologgingEventLogger\u001B[38;5;241m.\u001B[39mget_logger()\u001B[38;5;241m.\u001B[39mlog_original_function_success,\n\u001B[1;32m    472\u001B[0m         session,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    476\u001B[0m         og_kwargs,\n\u001B[1;32m    477\u001B[0m     )\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m original_fn_result\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:530\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001B[0;34m(*_og_args, **_og_kwargs)\u001B[0m\n\u001B[1;32m    522\u001B[0m \u001B[38;5;66;03m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001B[39;00m\n\u001B[1;32m    523\u001B[0m \u001B[38;5;66;03m# during original function execution, even if silent mode is enabled\u001B[39;00m\n\u001B[1;32m    524\u001B[0m \u001B[38;5;66;03m# (`silent=True`), since these warnings originate from the ML framework\u001B[39;00m\n\u001B[1;32m    525\u001B[0m \u001B[38;5;66;03m# or one of its dependencies and are likely relevant to the caller\u001B[39;00m\n\u001B[1;32m    526\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m set_non_mlflow_warnings_behavior_for_current_thread(\n\u001B[1;32m    527\u001B[0m     disable_warnings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    528\u001B[0m     reroute_warnings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    529\u001B[0m ):\n\u001B[0;32m--> 530\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m \u001B[43moriginal\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m_og_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m_og_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    531\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m original_result\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 52\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m pywrap_tfe\u001B[38;5;241m.\u001B[39mTFE_Py_Execute(ctx\u001B[38;5;241m.\u001B[39m_handle, device_name, op_name,\n\u001B[1;32m     53\u001B[0m                                       inputs, attrs, num_outputs)\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     55\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\n\u001B[0;31mUnimplementedError\u001B[0m: Graph execution error:\n\nDetected at node 'Cast_1' defined at (most recent call last):\n    File \"/databricks/python_shell/scripts/db_ipykernel_launcher.py\", line 145, in <module>\n      app.start()\n    File \"/databricks/python/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/databricks/python/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/databricks/python/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/databricks/python/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/databricks/python/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/databricks/python/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/databricks/python_shell/dbruntime/DatabricksShell.py\", line 98, in do_execute\n      reply_content = await super().do_execute(*args, **kwargs)\n    File \"/databricks/python/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/databricks/python/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/databricks/python/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"/databricks/python/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"/databricks/python/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/databricks/python/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/databricks/python/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/databricks/python/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/root/.ipykernel/982/command-2246144502270323-1672747808\", line 3, in <module>\n      (experimentID, runID) = run_mlflow()\n    File \"/root/.ipykernel/982/command-2246144502270322-1499505420\", line 40, in run_mlflow\n      model.fit(x_train, y_train, epochs=args.epochs, batch_size=args.batch_size)\n    File \"/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py\", line 550, in safe_patch_function\n      patch_function.call(call_original, *args, **kwargs)\n    File \"/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py\", line 168, in call\n      return cls().__call__(original, *args, **kwargs)\n    File \"/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py\", line 172, in __call__\n      return self._patch_implementation(original, *args, **kwargs)\n    File \"/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py\", line 230, in _patch_implementation\n      result = super()._patch_implementation(original, *args, **kwargs)\n    File \"/databricks/python/lib/python3.10/site-packages/mlflow/tensorflow/__init__.py\", line 1268, in _patch_implementation\n      history = original(inst, *args, **kwargs)\n    File \"/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py\", line 533, in call_original\n      return call_original_fn_with_event_logging(_original_fn, og_args, og_kwargs)\n    File \"/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py\", line 468, in call_original_fn_with_event_logging\n      original_fn_result = original_fn(*og_args, **og_kwargs)\n    File \"/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py\", line 530, in _original_fn\n      original_result = original(*_og_args, **_og_kwargs)\n    File \"/databricks/python/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/databricks/python/lib/python3.10/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/databricks/python/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/databricks/python/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/databricks/python/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/databricks/python/lib/python3.10/site-packages/keras/engine/training.py\", line 1028, in train_step\n      return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/databricks/python/lib/python3.10/site-packages/keras/engine/training.py\", line 1122, in compute_metrics\n      self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/databricks/python/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 605, in update_state\n      metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/databricks/python/lib/python3.10/site-packages/keras/utils/metrics_utils.py\", line 77, in decorated\n      update_op = update_state_fn(*args, **kwargs)\n    File \"/databricks/python/lib/python3.10/site-packages/keras/metrics/base_metric.py\", line 140, in update_state_fn\n      return ag_update_state(*args, **kwargs)\n    File \"/databricks/python/lib/python3.10/site-packages/keras/metrics/base_metric.py\", line 676, in update_state\n      y_true = tf.cast(y_true, self._dtype)\nNode: 'Cast_1'\nCast string to float is not supported\n\t [[{{node Cast_1}}]] [Op:__inference_train_function_5023]",
       "errorSummary": "<span class='ansi-red-fg'>UnimplementedError</span>: Graph execution error:\n\nDetected at node 'Cast_1' defined at (most recent call last):\n    File \"/databricks/python_shell/scripts/db_ipykernel_launcher.py\", line 145, in <module>\n      app.start()\n    File \"/databricks/python/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/databricks/python/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/databricks/python/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/databricks/python/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/databricks/python/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/databricks/python/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/databricks/python_shell/dbruntime/DatabricksShell.py\", line 98, in do_execute\n      reply_content = await super().do_execute(*args, **kwargs)\n    File \"/databricks/python/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/databricks/python/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/databricks/python/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"/databricks/python/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"/databricks/python/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/databricks/python/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/databricks/python/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/databricks/python/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/root/.ipykernel/982/command-2246144502270323-1672747808\", line 3, in <module>\n      (experimentID, runID) = run_mlflow()\n    File \"/root/.ipykernel/982/command-2246144502270322-1499505420\", line 40, in run_mlflow\n      model.fit(x_train, y_train, epochs=args.epochs, batch_size=args.batch_size)\n    File \"/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py\", line 550, in safe_patch_function\n      patch_function.call(call_original, *args, **kwargs)\n    File \"/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py\", line 168, in call\n      return cls().__call__(original, *args, **kwargs)\n    File \"/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py\", line 172, in __call__\n      return self._patch_implementation(original, *args, **kwargs)\n    File \"/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py\", line 230, in _patch_implementation\n      result = super()._patch_implementation(original, *args, **kwargs)\n    File \"/databricks/python/lib/python3.10/site-packages/mlflow/tensorflow/__init__.py\", line 1268, in _patch_implementation\n      history = original(inst, *args, **kwargs)\n    File \"/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py\", line 533, in call_original\n      return call_original_fn_with_event_logging(_original_fn, og_args, og_kwargs)\n    File \"/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py\", line 468, in call_original_fn_with_event_logging\n      original_fn_result = original_fn(*og_args, **og_kwargs)\n    File \"/databricks/python/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py\", line 530, in _original_fn\n      original_result = original(*_og_args, **_og_kwargs)\n    File \"/databricks/python/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/databricks/python/lib/python3.10/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/databricks/python/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/databricks/python/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/databricks/python/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/databricks/python/lib/python3.10/site-packages/keras/engine/training.py\", line 1028, in train_step\n      return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/databricks/python/lib/python3.10/site-packages/keras/engine/training.py\", line 1122, in compute_metrics\n      self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/databricks/python/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 605, in update_state\n      metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/databricks/python/lib/python3.10/site-packages/keras/utils/metrics_utils.py\", line 77, in decorated\n      update_op = update_state_fn(*args, **kwargs)\n    File \"/databricks/python/lib/python3.10/site-packages/keras/metrics/base_metric.py\", line 140, in update_state_fn\n      return ag_update_state(*args, **kwargs)\n    File \"/databricks/python/lib/python3.10/site-packages/keras/metrics/base_metric.py\", line 676, in update_state\n      y_true = tf.cast(y_true, self._dtype)\nNode: 'Cast_1'\nCast string to float is not supported\n\t [[{{node Cast_1}}]] [Op:__inference_train_function_5023]",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# corrida con parámetros diferentes a los por defecto\n",
    "args = parser.parse_args([\"--batch_size\", '256', '--epochs', '8'])\n",
    "(experimentID, runID) = run_mlflow()\n",
    "print(\"MLflow Run completed with run_id {} and experiment_id {}\".format(runID, experimentID))\n",
    "print(tf.__version__)\n",
    "print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eda8a952-32cb-426a-b8ff-83b40652ed79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ff85ec1-a17d-4abf-842b-38e5a942bba5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "MLflow-icfes",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}